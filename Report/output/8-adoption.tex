\section{\label{sec:adoption}Exploration of differences in adoption estimates on DSW villages}

This Section compares adoption estimates calculated using different methodologies and discusses potential reasons for the differences between them. This exploratory analysis is descriptive in nature and intended to assess the plausibility of different hypotheses for explaining the observed differences. It relies on subsamples of our data sets so we can compare like to like, but these subsamples are sometimes limited in size. While this approach helps discount some hypotheses, it cannot conclusively identify the causes behind the differences in estimates.

Evidence Action’s Q3 Monitoring estimates are roughly twice as high as our main estimates (Table \ref{tab:compare-adoption}). Section \ref{sec:adoption-ea} discusses several hypotheses that may explain this difference, though we lack sufficient data to test them adequately. Our Household Survey estimates represent 52\% and 58\% of Q3 Monitoring estimates in Malawi and Uganda, respectively—proportions consistent with Kenya’s SCM findings, where independent surveyors obtained chlorination rates representing 55\% of those calculated by Evidence Action. We discard priming and sample selection of water points in our main estimate as potential explanations for these differences, pointing to a combination of sampling variation, surveyor incentives, potential deviations from survey protocols as more likely causes.

In Malawi, or main estimates of chlorine adoption among households using water points with DSW included in  26.6\% (95\% CI = 21.1-32.1)\footnote{Table \ref{tab:headlines-vil}, column 7.} are not statistically from those obtained through the Monitoring methods 30.1\% (95\% CI = 24.5-35.7)\footnote{Table \ref{tab:headlines-am}}. This suggests that \textbf{although several factors may bias estimates obtained under the Monitoring methodology, a full census of served villages may not be necessary to obtain reliable adoption estimates}. In Uganda, however, the estimates from the same two surveys differ significantly (p-value = 0.0005), from a main estimate of 30.1\% (95\% CI = 24.5-35.7)\footnote{Table \ref{tab:headlines-vil}, column 7.} and to a Monitoring estimate of 0.1\% (95\% CI = 24.5-35.7)\footnote{Table \ref{tab:headlines-am}}. Section \ref{sec:adoption-dil} discusses potential explanations for this difference. A combination of sample selection, survey timing, and interactions with promoters is likely biasing Monitoring estimates upward in Uganda. 

\subsection{Potential explanations for differences between this report's and Evidence Action's estimates}\label{sec:adoption-ea} 

\subsubsection{Surveyor's independence}

A key methodological difference between DIL and Evidence Action protocols is who collects the data. Our estimates rely on data collected by independent enumerators hired by IPA, while Evidence Action uses their own local staff. This means surveyors and promoters may respond to different incentives:

\begin{itemize}
    \item Evidence Action surveyors may have existing relationships with promoters or implementation teams, creating incentives to portray them positively;
    \item Promoters were chosen by their communities to be the point of contact with Evidence Action and may seek to maintain positive relationships with the organization;
    \item Evidence Action staff may have interests in program continuation, which depends on positive results.
\end{itemize}

All these factors create potential incentives for surveyors and promoters to seek outcomes favorable to program success, which we would expect to bias Evidence Action estimates upward relative to those obtained via independent surveys.

Although we lack the data to test the impact of this factor on estimates, empirical evidence from Kenya supports the hypothesis that it does play a role. When Evidence Action examined identical households using both measurement approaches, the independent household survey found that 21\% of households tested positive for chlorine residual, while the Evidence Action monitoring survey reported 38\%. This means the independent survey estimate represented only 55\% of the Evidence Action figure.

Our results are consistent with the findings from Kenya (Table \ref{tab:compare-adoption}). In Malawi, the independent household survey estimate (26.6\%) represents 52\% of the Evidence Action monitoring figure (51\%). Similarly, in Uganda, the independent estimate (30.1\%) represents 58\% of the Evidence Action monitoring result (52\%).

\subsubsection{Sample selection of water points}

The water points included in Evidence Action’s Adoption Monitoring Survey and our Promoter Survey are systematically different due to contrasting treatment of water points whose promoter could not be surveyed.

Our protocol specified that only promoters (and ILC operators in ILC villages) should be surveyed and asked to provide a list of water point users. When Evidence Action did not provide contact information for a promoter or one could not be identified by asking villagers, no user list was obtained and no households were sampled from that list to respond to the Household Survey. Evidence Action, by contrast, surveys any knowledgeable village member when the promoter cannot be found.

This difference in sampling strategy has implications for the representativeness of our data and may bias estimates. However, the expected direction of bias works against explaining the observed gap:

\begin{itemize}
    \item Evidence Action’s broader respondent pool likely produces more representative estimates of overall program reach.
    \item Our promoter-focused approach would be expected to introduce upward bias into estimates following the monitoring method, as promoters are better positioned than typical village members to identify both households using the water points and households that chlorinate their water, and may be more motivated to prioritize successfully adopting households when selecting survey respondents. The promoters who can be more easily identified are also more likely to be in touch with Evidence Action and, presumably, to be conducting promotion activities in the village, which should lead to higher chlorine adoption rates.
\end{itemize}

Given that our Monitoring estimates are lower than Evidence Action’s Q3 Monitoring estimates, and that in Malawi—where coverage is more representative—Monitoring Survey estimates are closer to Household Survey estimates, the difference in water point representativeness is unlikely to explain the overall divergence in estimates.

\subsubsection{Survey timing and duration}\label{sec:adoption-priming}

Both ours and Evidence Action protocols stipulate that promoters should only be contacted when the surveyor is ready ready to conduct the interview. However, the survey activities differed substantially in duration: Evidence Action monitoring staff spent only one day in each village, while IPA teams remained for an average of five days, conducting monitoring surveys on the final day.

\textit{The extended presence of IPA field teams may bias chlorine adoption estimates in both the Household and Monitoring Surveys upward}, as promoters have opportunities to refill dispensers and remind households to add chlorine to their drinking water and the village population becomes more aware of water treatment practices. Note, however, that this finding should \textit{decrease} the difference between Evidence Action's estimates and ours. \textit{Therefore, it does not help us explain the observed results. }

\subsubsection{Other possible causes of divergence}

The following factors may contribute to the differences, but we are unable to test them adequately:

\begin{itemize}
    \item \textbf{Water point sampling variation:} IPA and Evidence Action did not visit the same water points, and sampling variation may explain part of the difference.
    \item \textbf{Time since training of surveyors:} IPA surveyors were trained between one week and four months before interviews took place, but the timing of Evidence Action staff retraining is unknown. This would introduce noise into measurements, but the direction of any bias is unclear, as more experience may lead surveyors to develop better skills and judgement or to forget protocols and develop of bad practices.
    \item \textbf{State of equipment:} The equipment used in our data collection was procured specifically for this exercise, and all reagents were new. We lack data about how long Evidence Action’s equipment has been in use or when reagents were procured. The use of old materials could bias estimates in either direction -- for example, discolored color wheels would bias estimates upward, while degraded DPD reagents would bias estimates downward.
    \item \textbf{Deviations from randomization protocol:} Household randomization into our Household Survey was recorded in the data, allowing us to observe whether sampled households were replaced. In Evidence Action’s protocol, this is recorded on paper, leaving more room for enumerators to choose whom they survey (e.g., households closer to the promoter’s house or households indicated by the promoter).

\end{itemize}

\subsection{Potential explanations for differences estimates obtained using the Census and the Monitoring methods}\label{sec:adoption-dil}

This section discusses potential explanations for the differences in chlorine residual detection rates observed between Table \ref{tab:headlines-vil} and Table \ref{tab:headlines-am}.

As expected given the similarity between Monitoring and Household Survey estimates in Malawi, there is no meaningful variation in chlorine residual detection rates across subsamples in that country. In Uganda, the estimates in Figure 2 are significantly different (p-value = 0.053) and point estimates in Household Survey subsamples become more similar to Monitoring Survey estimates as we apply similar sample selection criteria. However, there is considerable noise in estimates across the subsamples in Figure 3, and they are not significantly different from one another. As such, the discussion in this Section remains inconclusive.

\textcolor{red}{Multiple potential sources of bias exist, and this analysis is merely observational.

We observe sample selection into the promoter list using census-level data, even if not reflected in the Household Survey after randomization. Therefore, we cannot rule it out as a potential source of differences between the Household Survey and the Adoption Monitoring estimates. We also only observe differences between Monitoring and Household Survey estimates in Uganda — precisely where we know interference occurred — suggesting this as a potential cause of differences in estimates given the lack of support to other explanations. Enumerator identity (reflecting differences in bias, training, and equipment) is also likely to play a role.

We lack definitive evidence from this data alone to conclude that obtaining lists from promoters should be avoided. However, given that Monitoring Survey estimates in both Kenya and Uganda were higher than Household Survey estimates, there is some evidence pointing in that direction.}

\subsection{Survey timing and duration}

As discussed in Section \ref{sec:adoption:priming}, the duration of activities in the village could bias adoption estimates upward. Because surveying households from the promoter list sample was the last activity carried out in each village, taking place after promoters were surveyed, the potential for bias is exacerbated in these estimates.

The ideal test to determine if timing biases estimates would compare the same households surveyed in both the Household and Monitoring Surveys. However, given our protocol, too few such observations exist for meaningful analysis. Instead, we compare chlorine detection rates using data from Household Survey observations of households sampled from the census. Among these household, those tested after the first day of the survey showed higher rates of TCR detection in both Malawi and Uganda (Figure \ref{fig:adoption-date}). Although there is limited statistical support for this finding (the difference is not significant in Malawi and in Uganda, although there is a difference of almost 10 percentage points, it is only significant at the 10\% confidence level), \textbf{it is consistent with our expectations that extended survey activities can lead to priming}. 

\subsection{Sample selection of water points}

When designing survey protocols, we intended for estimates following both the census and the monitoring methods to be representative of all water points with DSW in sampled villages. However, as discussed in Section \ref{}, the field team was unable to identify promoters for all water points found, meaning water points covered included in Monitoring estimates are systematically different from and less representative than those in the Census estimates.

This is reflected in the chlorine detection rates observed among households in the Household Survey: in both Malawi and Uganda, we estimates these rates to be around 16\% among households using water points with DSW whose promoters were not surveyed, compared to approximately 30\% among households using water points included in the promoter survey (Figure \ref{fig:adoption-selection-wp}). The difference is significant in both countries (p-value = 0.057 in Malawi and 0.0005 in Uganda). However, since there are fewer households using water points excluded than included in the promoter survey (7\% in Malawi and 20\% in Uganda), this difference does not explain much of the difference between adoption estimates based on the census and the monitoring methods.

\subsection{Sample selection of households}

The list of households using water points provided by promoters is another potential source of difference. Promoters are expected to have less complete knowledge than households about water point usage patterns, and they also respond to incentives.

Promoters’ reduced knowledge would introduce noise into the data. For example, in some cases promoters relied on lists of water point users recorded in previous years, dating back as far as dispenser installation. Several additional factors could bias estimates in different directions:

\begin{itemize}
    \item If a household uses more than one water point, promoters may not know whether a water point is the household’s primary drinking water source, and including households whose primary source lacks DSW would bias estimates downward;
    \item If promoters wish to demonstrate that a water point is widely used and should continue being treated, the list could include households whose primary source lacks DSW;
    \item If promoters remember mostly people with whom they have closer relationships or to whom they recently discussed chlorination, the list could lead to overestimation of chlorination rates;
    \item If promoters expect surveyors to speak with listed households and wish to demonstrate good performance, the list will more likely include households known to use chlorine, biasing estimates upward Promoters would be expected to list all households using the water point regardless of location, while the household survey limits its sample to households within or up to 200m outside village boundaries.
\end{itemize}

\textit{The data shows that promoters are effective at identifying households using water points with dispensers.} In Malawi, out of the rough five thousand households included in the promoter list who were identified in the household census, approximately 70\% self-report using a water point with DSW as their primary source of drinking water. In Uganda, this figure was 75\% out of four thousand households.

\textit{However, promoters are less effective at identifying all of the users.} Among the households mapped in the household census who self-reported using water points in the promoter survey as their primary source of drinking water, nearly 80\% are included in the promoter list in Malawi, while in Uganda they identified only half of the users.


Among households that self-report using water points listed by the promoter, those included in thethe promoter list are significantly different from those that are not (Tables 4 and 5), including with regard to chlorine access. However, when we further restrict the sample to households included in the Household Survey, these differences are not jointly significant. This finding is consistent with results in Figures 3 and 5. Figure 3 shows that restricting the Household Survey sample to households included in the promoter list does not lead to different adoption estimates, and Figure 5 confirms this by showing that TCR detection is similar among households included in and excluded from the list.

Although this does not point to differences between the Household Survey and Monitoring Survey estimates, the fact that differences exist between listed and not listed households observed in the Household Census indicates that inclusion in the promoter list does play a role and may help explain divergences from Evidence Action estimates.



\subsection{Other possible explanations}

\begin{itemize}
    \item \textbf{Village boundaries:} As discussed in \ref{}, while the Household Census restricts listed households to those living within 200m from the village boundaries, the Promoter Survey included anyone using the water points regardless of their location. However, this is not what we observe on the data: around 7\% of the households in the Household Census that were also in a promoter list live outside of the village boundaries, compared to roughly 10\% among households not listed by a promoter. In the Household Survey, Chlorine detection rates are lower among household living outside of village boundaries (Figure \ref{fig:adoption-boundaries}), and there are more such households in the census sample (17\% of surveyed households) than in the monitoring sample (7\%), which could lower census estimates and explain part of the difference in estimates observed. However, there is still a 12 percentage point difference between census and Monitoring estimates in Uganda when restricting comparison to households within village boundaries (p-value = 0.001).
    \item \textbf{Replacement protocol:} During the Household Survey, households were only replaced after three failed attempts at a visit. For the Monitoring Survey, the protocol indicated that households could be replaced for two reasons:
    \begin{enumerate}
        \item They had already been surveyed during the Household Survey; or
        \item They were unavailable when visited for the Monitoring Survey. This could lead to bias if households where a member is more likely to be found at home during the data are also more likely to chlorinate or to have collected water more recently.
    \end{enumerate}
    \textcolor{red}{Figure 7 compares chlorine detection rates among households sampled for the Monitoring Survey and replaced because they had already been surveyed in the Household Survey versus replacement households in the Monitoring Survey. These differences are not statistically significant, but they are relatively large in Uganda. In Malawi, most replacements occurred because households had already been surveyed in the Household Survey, while in Uganda, approximately 40\% of replacements occurred because households were unavailable.}
\end{itemize}